{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following pacakges are being used for the following reasons:  \n",
    "**Pandas**: Used to build a data frame, and export the data in CSV format.  \n",
    "**Numpy**: Used to manipulate arrays of data that get passed on to pandas to build a data frame.  \n",
    "**Requests**: Used to make get requests to websites and APIs in order to get the necessary data.  \n",
    "**BeautifulSoup**: Used to parse html files, and extract text.  \n",
    "**Json**: Used to manipulate jsons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The website used to gather box office data is 'The Numbers'. The top 500 movies are gather from the website, so 'pages' stores a list of all the suffixes needed to access the data.  \n",
    "  \n",
    "The for loop iterates over the pages list. For every item in the list, the loop parses the HTML file and adds the 'tbody' tag to the empty list tables. After it has iterated over all the items, it returns the tables list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTables():\n",
    "    tables = []\n",
    "    url = 'https://www.the-numbers.com/box-office-records/worldwide/all-movies/cumulative/all-time'\n",
    "    pages = ['','/101','/201','/301','/401']\n",
    "    \n",
    "    for i in pages:\n",
    "        page = requests.get(url+i).content\n",
    "        soup = bs(page,'html.parser')\n",
    "        table = soup.find('tbody')\n",
    "        tables.append(table)\n",
    "    \n",
    "    return tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in a tables arguement, which is the list returned by the getTables() function. The data varibla stores an empty list, which will be used to append text after it is extracted. A for loop is used to iterate over the tables list that has been taken as an arguement, finds all the 'td' tags, and extracts the text. The data list is returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(tables):\n",
    "    data = []\n",
    "    for i in tables:\n",
    "        section = i.find_all('td')\n",
    "        for i in section:\n",
    "            text = i.get_text()\n",
    "            data.append(text)\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a list of all the extracted test from the getText() function. After, it uses numpy to split the list into 500 equal parts. This is passed into pandas to build a dataframe, and it drops the index column that came from scrapping the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataFrame(data):\n",
    "    data = np.array_split(data,500)\n",
    "    df = pd.DataFrame(data,columns=['index','year','movie','worldwide','domestic','international'])\n",
    "    df.drop('index',axis=1,inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to fetch the API key for omdb. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys(path):\n",
    "    with open(path) as f:\n",
    "        key = json.load(f)\n",
    "    return key['omdb_key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes two arguements. The first arguement key is the API key for omdb, and the second is the dataframe returned by buildDataFrame(). It creates 5 empty lists to store different attributes of the 500 movies. The for loop iterates over the dataframe's movie column, uses a get method from the requests library to fetch a json from omdb, and it checks if the response returned data. If the call returned data, it will access the data and append it to the corresponding list. Also, it checks if there is a rating from rotten tomatoes. If there is no rating, it will append a 'NaN' to the list. If the call did not returned data, it appends 'NaN' to the corresponding list. After, the lists get appended onto the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInfo(key,df):\n",
    "    rated = []\n",
    "    genre = []\n",
    "    director = []\n",
    "    writer = []\n",
    "    critics = []\n",
    "    production = []\n",
    "    \n",
    "    for i in df.movie:\n",
    "        i = i.replace(\" \",\"+\")\n",
    "        url = \"http://www.omdbapi.com/?apikey={}&t={}\".format(key,i)\n",
    "        data = requests.get(url).json()\n",
    "        \n",
    "        if data.get('Response') == 'True':\n",
    "            rated.append(data.get('Rated'))\n",
    "            genre.append(data.get('Genre'))\n",
    "            director.append(data.get('Director'))\n",
    "            writer.append(data.get('Writer'))\n",
    "            try:\n",
    "                critics.append(data.get('Ratings')[1]['Value'])\n",
    "            except:\n",
    "                critics.append('NaN')\n",
    "            production.append(data.get('Production'))\n",
    "        else:\n",
    "            rated.append('NaN')\n",
    "            genre.append('NaN')\n",
    "            director.append('NaN')\n",
    "            writer.append('NaN')\n",
    "            critics.append('NaN')\n",
    "            production.append('NaN')\n",
    "          \n",
    "    df['rated'] = rated\n",
    "    df['genre'] = genre\n",
    "    df['director'] = director\n",
    "    df['writer'] = writer\n",
    "    df['critics'] = critics\n",
    "    df['production'] = production\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the code combines all the functions, and it uses the pandas to_csv() method in order to save a copy of the dataframe for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = getTables()\n",
    "data = getText(tables)\n",
    "df = buildDataFrame(data)\n",
    "key = get_keys('.secret/keys.json')\n",
    "finalDF = getInfo(key,df)\n",
    "finalDF.to_csv('data/movie_data.csv',index=False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
